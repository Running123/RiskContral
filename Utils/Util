import datetime as dt
import pandas as pd
import numpy as np
from pandas import DataFrame,Series
#import json
import urllib2
import urllib
from pylab import *
import MySQLdb
import xlrd
from lxml.html import parse
from sklearn import tree
from sklearn.externals.six import StringIO
import pydot 
from IPython.display import Image  
from sklearn.cross_validation import cross_val_score

def getDataRange(datarange,starttime=dt.datetime.now().strftime('%Y%m%d')):
    nowtime=dt.datetime.strptime(starttime,'%Y%m%d');
    for i in range(1,datarange+1)[::-1]:
        print (nowtime - dt.timedelta(days =i)).strftime('%Y%m%d')+',',



def plotTrend(a):
    x=[dt.datetime.strptime(i.split(':')[0],'%H%M') for i in a]
    y=[int(i.split(':')[1]) for i in a]
    plot(x,y,'-')
    data=pd.Series(y,index=x)
    data.plot()


def getRTBbasic(ip=None):
    apiKey="1b4bdd8afcc23d4157ec35ab6ccebde5"
    
    if ip is None:
        url = 'http://apis.baidu.com/rtbasia/non_human_traffic_screening_vp/nht_query?ip=117.143.239.154'
    else:
        url = 'http://apis.baidu.com/rtbasia/non_human_traffic_screening_vp/nht_query?ip='+ip
        
    req = urllib2.Request(url)
    req.add_header("apikey",apiKey)
    
    resp = urllib2.urlopen(req)
    content = resp.read()
    if(content):
        print(content)

def getWebdata(url='https://passport.suning.com/ids/login'):
    head={'User-Agent':'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT) liugao'}
    data={'method':'GET','loginTheme':'b2c'}
    value=urllib.urlencode(data)
    req=urllib2.Request(url,value,head)
    
    try:
        respond=urllib2.urlopen(req)
        return respond
    except URLError,e:
        print e
        return 0
        
def getDatabases():
    db = MySQLdb.connect("localhost","suning","1990","test" )
    cursor = db.cursor()
    cursor.execute('select * from price')
    data = cursor.fetchall()
    price=np.array(data)
    price=pd.DataFrame(price,columns=['id','gds_cd','prices','datatime'])
    db.close()
    return price

def getDataExcel(files,sheet='Sheet1'):
    xls_file=pd.ExcelFile(files)
    table=xls_file.parse(sheet,parse_dates=True,index_col='date')
    return table

def getWafModel(files):
    columns_name=['ip','x_ip','v_tm','v_count','page','all','ref','ua','mem','custmid','dsi','host','date']
    ip=pd.read_table(files,names=columns_name)
    dataset=np.array(ip)[:,1:]
    for i in range(len(dataset)):
        dataset[i][11]=np.where(dataset[i][10]>=10 or dataset[i][1]>=600 or dataset[i][2]>=100,1,0).item()
    dataset=dataset.astype('int64')
    #trainip=[]
    #testip=[]
    #for i in range(len(dataset)):
    #    if i%4==0:
    #        testip.append(dataset[i])
    #    else:
    #        trainip.append(dataset[i])
    #trainip=array(trainip)
    #testip=array(testip)
            
    #clf = tree.DecisionTreeRegressor()
    clf = tree.DecisionTreeClassifier()
    score=cross_val_score(clf, dataset[:,0:11], dataset[:,11], cv=10)
    
    print 'the socre of model: %s'%score
    
    clf = clf.fit(dataset[:,0:11], dataset[:,11])
    dot_data = StringIO() 
    tree.export_graphviz(clf, out_file=dot_data,feature_names=columns_name[1:12],  
                         class_names=columns_name[12],  
                         filled=True, rounded=True,  
                         special_characters=True) 
    graph = pydot.graph_from_dot_data(dot_data.getvalue())
    graph.write_pdf("tree.pdf")

    return clf


from pyspark.sql import HiveContext
hc=HiveContext(sc)
data_waf=hc.sql("select * from safe.snwaf_acess_log")
